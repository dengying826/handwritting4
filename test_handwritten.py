import util
from bean import autoencoder, nn
import numpy as np
import load_dataset
import matplotlib.pyplot as plt

# 测试数据，x为输入，y为输出
# x = np.array([
# [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.3803922,0.37647063,0.3019608,0.46274513,0.2392157,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.3529412,0.5411765,0.9215687,0.9215687,0.9215687,0.9215687,0.9215687,0.9215687,0.9843138,0.9843138,0.9725491,0.9960785,0.9607844,0.9215687,0.74509805,0.08235294,0,0,0,0,0,0,0,0,0,0,0,0.54901963,0.9843138,0.9960785,0.9960785,0.9960785,0.9960785,0.9960785,0.9960785,0.9960785,0.9960785,0.9960785,0.9960785,0.9960785,0.9960785,0.9960785,0.9960785,0.7411765,0.09019608,0,0,0,0,0,0,0,0,0,0,0.8862746,0.9960785,0.81568635,0.7803922,0.7803922,0.7803922,0.7803922,0.54509807,0.2392157,0.2392157,0.2392157,0.2392157,0.2392157,0.5019608,0.8705883,0.9960785,0.9960785,0.7411765,0.08235294,0,0,0,0,0,0,0,0,0,0.14901961,0.32156864,0.0509804,0,0,0,0,0,0,0,0,0,0,0,0.13333334,0.8352942,0.9960785,0.9960785,0.45098042,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.32941177,0.9960785,0.9960785,0.9176471,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.32941177,0.9960785,0.9960785,0.9176471,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.4156863,0.6156863,0.9960785,0.9960785,0.95294124,0.20000002,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.09803922,0.45882356,0.8941177,0.8941177,0.8941177,0.9921569,0.9960785,0.9960785,0.9960785,0.9960785,0.94117653,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.26666668,0.4666667,0.86274517,0.9960785,0.9960785,0.9960785,0.9960785,0.9960785,0.9960785,0.9960785,0.9960785,0.9960785,0.5568628,0,0,0,0,0,0,0,0,0,0,0,0,0,0.14509805,0.73333335,0.9921569,0.9960785,0.9960785,0.9960785,0.8745099,0.8078432,0.8078432,0.29411766,0.26666668,0.8431373,0.9960785,0.9960785,0.45882356,0,0,0,0,0,0,0,0,0,0,0,0,0.4431373,0.8588236,0.9960785,0.9490197,0.89019614,0.45098042,0.34901962,0.12156864,0,0,0,0,0.7843138,0.9960785,0.9450981,0.16078432,0,0,0,0,0,0,0,0,0,0,0,0,0.6627451,0.9960785,0.6901961,0.24313727,0,0,0,0,0,0,0,0.18823531,0.9058824,0.9960785,0.9176471,0,0,0,0,0,0,0,0,0,0,0,0,0,0.07058824,0.48627454,0,0,0,0,0,0,0,0,0,0.32941177,0.9960785,0.9960785,0.6509804,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.54509807,0.9960785,0.9333334,0.22352943,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.8235295,0.9803922,0.9960785,0.65882355,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.9490197,0.9960785,0.93725497,0.22352943,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.34901962,0.9843138,0.9450981,0.3372549,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.01960784,0.8078432,0.96470594,0.6156863,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.01568628,0.45882356,0.27058825,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
# [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.12156864,0.5176471,0.9960785,0.9921569,0.9960785,0.8352942,0.32156864,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.08235294,0.5568628,0.91372555,0.98823535,0.9921569,0.98823535,0.9921569,0.98823535,0.8745099,0.07843138,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.48235297,0.9960785,0.9921569,0.9960785,0.9921569,0.87843144,0.7960785,0.7960785,0.8745099,1,0.8352942,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.7960785,0.9921569,0.98823535,0.9921569,0.8313726,0.07843138,0,0,0.2392157,0.9921569,0.98823535,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.16078432,0.95294124,0.87843144,0.7960785,0.7176471,0.16078432,0.59607846,0.11764707,0,0,1,0.9921569,0.40000004,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.15686275,0.07843138,0,0,0.40000004,0.9921569,0.19607845,0,0.32156864,0.9921569,0.98823535,0.07843138,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.32156864,0.83921576,0.12156864,0.4431373,0.91372555,0.9960785,0.91372555,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.24313727,0.40000004,0.32156864,0.16078432,0.9921569,0.909804,0.9921569,0.98823535,0.91372555,0.19607845,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.59607846,0.9921569,0.9960785,0.9921569,0.9960785,0.9921569,0.9960785,0.91372555,0.48235297,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.59607846,0.98823535,0.9921569,0.98823535,0.9921569,0.98823535,0.75294125,0.19607845,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.24313727,0.7176471,0.7960785,0.95294124,0.9960785,0.9921569,0.24313727,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.15686275,0.6745098,0.98823535,0.7960785,0.07843138,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.08235294,0,0,0,0,0,0,0,0,0,0.7176471,0.9960785,0.43921572,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.24313727,0.7960785,0.6392157,0,0,0,0,0,0,0,0,0.2392157,0.9921569,0.5921569,0,0,0,0,0,0,0,0,0,0,0,0,0,0.08235294,0.83921576,0.75294125,0,0,0,0,0,0,0,0,0.04313726,0.8352942,0.9960785,0.5921569,0,0,0,0,0,0,0,0,0,0,0,0,0,0.40000004,0.9921569,0.5921569,0,0,0,0,0,0,0,0.16078432,0.8352942,0.98823535,0.9921569,0.43529415,0,0,0,0,0,0,0,0,0,0,0,0,0,0.16078432,1,0.8352942,0.36078432,0.20000002,0,0,0.12156864,0.36078432,0.6784314,0.9921569,0.9960785,0.9921569,0.5568628,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.6745098,0.98823535,0.9921569,0.98823535,0.7960785,0.7960785,0.91372555,0.98823535,0.9921569,0.98823535,0.9921569,0.50980395,0.07843138,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.08235294,0.7960785,1,0.9921569,0.9960785,0.9921569,0.9960785,0.9921569,0.9568628,0.7960785,0.32156864,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.07843138,0.5921569,0.5921569,0.9921569,0.67058825,0.5921569,0.5921569,0.15686275,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
# [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.02352941,0.76470596,0.9960785,1,0.93725497,0.1137255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.02352941,0.6392157,0.9960785,0.9960785,0.6862745,0.21568629,0.01176471,0,0,0,0,0.09019608,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.47450984,0.9960785,0.9960785,0.5803922,0.03137255,0,0,0,0,0,0.54901963,0.9294118,0.43921572,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.16862746,0.9607844,0.9960785,0.9490197,0.01568628,0,0,0,0,0,0.20784315,0.92549026,0.9960785,0.43921572,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.6431373,0.9960785,0.9803922,0.2509804,0,0,0,0,0,0,0.6627451,0.9960785,0.97647065,0.1254902,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.83921576,0.9960785,0.69803923,0,0,0.01176471,0.03529412,0.03529412,0.03529412,0.17254902,0.909804,0.9960785,0.64705884,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.83921576,0.9960785,0.7254902,0,0.21176472,0.7294118,0.9960785,0.9960785,0.9960785,0.9960785,0.9960785,0.9058824,0.16862746,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.37647063,0.9960785,0.9803922,0.74509805,0.98823535,0.9960785,0.9960785,0.9960785,0.9960785,0.9960785,0.9960785,0.50980395,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.49411768,0.9960785,0.9960785,0.9960785,0.9960785,0.7803922,0.46274513,0.5686275,0.9960785,0.9960785,0.9960785,0.30588236,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.10980393,0.47058827,0.47058827,0.47058827,0.1254902,0.01176471,0,0.47450984,0.9960785,0.9960785,0.76470596,0.01568628,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.43137258,0.9960785,0.9960785,0.33333334,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.41960788,0.9960785,0.98823535,0.19215688,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.8235295,0.9960785,0.7176471,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.05882353,0.87843144,0.9960785,0.2784314,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.27058825,0.9960785,0.97647065,0.20784315,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.8470589,0.9960785,0.92549026,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.8745099,0.9960785,0.74509805,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.8745099,0.9960785,0.73333335,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.8745099,0.9960785,0.97647065,0.49411768,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.44705886,0.9333334,0.9960785,0.48627454,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
# ])
# # load_dataset.plot_image(x[1], 28)
# y = np.array([
#             [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
#             [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
#             [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]
#             ])
#################################
inputs, labels = load_dataset.load_data('data/mnist.pkl.gz')
num = 100
x = inputs[0:1000]
y = labels[0:1000]

plt.figure(figsize=(10, 10))
for i in range(100):
    # plt.subplots_adjust(wspace=0, hspace=0)
    plt.subplot(10, 10, i + 1)
    plt.imshow(x[i].reshape(28, 28), cmap='binary')
    plt.axis('off')
    plt.tight_layout()
plt.savefig('image/train.png')
plt.show()
#################################
# step1 建立autoencoder
# 弄两层autoencoder，其中784为输入的维度
nodes = [784,100,25]
# 建立auto框架
ae = util.aebuilder(nodes)
# 设置部分参数
# 训练，迭代次数为6000
ae = util.aetrain(ae, x, 3000)
##############################

# step2 微调
# 建立完全体的autoencoder，最后层数10为输出的特征数
nodescomplete = np.array([784, 100, 25, 10])
aecomplete = nn(nodescomplete)
# 将之前训练得到的权值赋值给完全体的autoencoder
# 训练得到的权值，即原来的autoencoder网络中每一个autoencoder中的第一层到中间层的权值
for i in range(len(nodescomplete) - 2):
    aecomplete.W[i] = ae.encoders[i].W[0]
# 开始进行神经网络训练，主要就是进行微调

# print("ae:",len(ae.encoders[0].W[0]))
# print("ae:",len(ae.encoders[0].W[0][0]))

# ae0 = np.zeros((784,49))
# for i in range(49):
#     for j in range(784):
#         ae0[i][j] = ae.encoders[0].W[0][j][i]
#     load_dataset.plot_image(ae0[i], 28, 1, i)
ae0 = ae.encoders[0].W[0]
ae0 = ae0.transpose()
plt.figure(figsize=(15, 15))
for i in range(100):
    # plt.subplots_adjust(wspace=0, hspace=0)
    plt.subplot(10, 10, i + 1)
    # temp = load_dataset.normlize_image(ae0[i])
    plt.imshow(ae0[i].reshape(28, 28), cmap='binary')
    plt.axis('off')
    plt.tight_layout()
plt.savefig('image/hidden1.png')
plt.show()

# ae1 = ae.encoders[1].W[0]
# ae1 = ae1.transpose()
# for i in range(36):
#     # plt.subplots_adjust(wspace=0, hspace=0)
#     plt.subplot(6,6,i+1)
#     plt.imshow(ae1[i].reshape(7,7),cmap='binary')
#     plt.axis('off')
#     plt.tight_layout()
# plt.savefig('image/hidden2.png')
# plt.show()

# for i in range(49):
#     n = ae1[0][i]
#     ae0[i] = ae0[i]*n


aecomplete = util.nntrain(aecomplete, x, y, 3000, True)
# w = np.zeros((49,784))
# for i in range(49):
#     for j in range(784):
#         w[i][j] = aecomplete.W[0][j][i]
# load_dataset.plot_image(w[i], 28, 1, i)


# 打印出最后一层的输出
print(aecomplete.values[3][0])

w = aecomplete.W[0]
w = w.transpose()
plt.figure(figsize=(15, 15))
for i in range(100):
    # plt.subplots_adjust(wspace=0, hspace=0)
    plt.subplot(10, 10, i + 1)
    # temp = load_dataset.normlize_image(ae0[i])
    plt.imshow(w[i].reshape(28, 28), cmap='binary')
    plt.axis('off')
    plt.tight_layout()
plt.savefig('image/hidden1_.png')
plt.show()
